import os
import requests
import streamlit as st

import joblib
import re
import string
import pandas as pd
import numpy as np
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.base import BaseEstimator, TransformerMixin
from dotenv import load_dotenv
from openai import OpenAI

st.set_page_config(
    page_title="Fake News Detector",
    page_icon="üì∞",
    layout="centered",
    initial_sidebar_state="expanded"
)


st.markdown("""
    <style>
    .stApp { background: #f8f9fa; }
    h1 { color: #2a4a5c; border-bottom: 2px solid #2a4a5c; padding-bottom: 0.3rem; }
    .stTextArea textarea { 
        border: 2px solid #4a90e2 !important; 
        border-radius: 8px !important;
        padding: 1rem !important;
    }
    .stButton button {
        background: #4a90e2 !important;
        color: white !important;
        border-radius: 25px;
        padding: 0.5rem 2rem;
        transition: all 0.3s;
    }
    .stButton button:hover {
        transform: scale(1.05);
        box-shadow: 0 4px 12px rgba(74,144,226,0.3);
    }
    .result-card {
        padding: 1.5rem;
        border-radius: 12px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }
    [data-testid="stSidebar"] {
        background: #2a4a5c !important;
        color: white !important;
    }
    </style>
""", unsafe_allow_html=True)

# --- Securely load API key ---
load_dotenv("api.env")
openai_api_key = os.getenv("OPEN_AI_API_KEY")
factcheck_api_key = os.getenv("FACT_CHECK_API_KEY")
API_KEY = factcheck_api_key
API_URL = "https://factchecktools.googleapis.com/v1alpha1/claims:search"
client = OpenAI(api_key=openai_api_key)

# --- Text preprocessing ---
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'\d+', '', text)
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

class TextCleaner(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        if not isinstance(X, pd.Series):
            X = pd.Series(X)
        return X.apply(clean_text)

# --- Cached Fact-checking functions ---
@st.cache_data(ttl=3600, show_spinner="Checking facts...")
def google_fact_check(claim_text):
    if not API_KEY:
        st.error("Fact Check API key not set. Please set FACT_CHECK_API_KEY in your environment.")
        return []
    params = {
        "query": claim_text,
        "languageCode": "en-US",
        "pageSize": 5,
        "key": API_KEY
    }
    try:
        response = requests.get(API_URL, params=params, timeout=10)
        response.raise_for_status()
        return response.json().get("claims", [])
    except requests.exceptions.RequestException as e:
        st.error(f"Fact-check API error: {str(e)}")
        return []

def parse_fact_check_results(claims):
    verified_claims = []
    for claim in claims:
        claim_text = claim.get("text", "N/A")
        claim_reviews = claim.get("claimReview", [])
        for review in claim_reviews:
            publisher = review.get("publisher", {}).get("name", "Unknown")
            review_url = review.get("url", "N/A")
            verdict = review.get("textualRating", "No verdict")
            verified_claims.append({
                "claim": claim_text,
                "publisher": publisher,
                "verdict": verdict,
                "url": review_url
            })
    return verified_claims

@st.cache_data(ttl=1800, show_spinner="Extracting claims...")
def extract_claims(text):
    """Extract factual claims from text using OpenAI's GPT model with caching"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{
                "role": "user",
                "content": f"""
                    Extract factual claims from this text as bullet points.
                    Focus on statements that can be fact-checked (e.g., dates, events, actions).
                    Return ONLY the claims, one per line.
                    Text: {text}
                """
            }]
        )
        claims = response.choices[0].message.content.split("\n")
        return [claim.strip("- ") for claim in claims if claim.strip()]
    except Exception as e:
        st.error(f"Error extracting claims: {str(e)}")
        return [text]


# --- Main Interface ---
with st.container():
    col1, col2 = st.columns([0.8, 0.2])
    with col1:
        st.title("üì∞ Fake News Detector")
    st.markdown("""
        <div style="margin: 1rem 0; font-size: 1.1rem; color: #4a4a4a;">
        This web app uses advanced machine learning and real-time fact-checking to analyze news content.
        </div>
    """, unsafe_allow_html=True)

# --- Load Model ---
@st.cache_resource(show_spinner="Loading model...")
def load_pipeline():
    return joblib.load("pipeline_with_linearSVC.joblib")
pipeline = load_pipeline()

# --- Prediction Section ---
with st.form(key='prediction_form'):
    user_text = st.text_area(
        "Paste your news article or headline here:",
        height=200,
        placeholder="Enter news content here..."
    )
    submit_col, _ = st.columns([0.2, 0.8])
    with submit_col:
        submitted = st.form_submit_button("üîç Analyze Content", use_container_width=True)

# --- Results Display ---
if submitted:
    if not user_text.strip():
        st.warning("Please enter some text to analyze.")
    else:
        with st.spinner("Analyzing content..."):
            # 1. Model prediction
            prediction = pipeline.predict([user_text])[0]
            try:
                proba = pipeline.decision_function([user_text])[0]
                confidence = 1 / (1 + np.exp(-proba))
            except Exception:
                confidence = 0.99 if prediction == 1 else 0.99  # fallback
            
            # 2. Extract claims
            claims_to_check = extract_claims(user_text)
            
            # 3. Fact-check claims
            fact_check_results = []
            for claim in claims_to_check:
                if claim.strip():
                    claims = google_fact_check(claim)
                    fact_check_results.extend(parse_fact_check_results(claims))
            
            # 4. Determine final label
            has_false_claim = any(
                result["verdict"].lower() == "false" for result in fact_check_results
            )
            final_label = "FAKE news" if prediction == 1 or has_false_claim else "REAL news"

        # --- Results Container ---
        with st.container():
            st.markdown(f"""
                <div class='result-card' style='background: {"#ffe6e6" if final_label == "FAKE news" else "#e6f4ea"};'>
                    <h3 style='color: {"#cc0000" if final_label == "FAKE news" else "#228B22"}; margin:0;'>
                        {final_label.upper()}
                    </h3>
                    <div style='margin-top: 0.5rem; color: #4a4a4a;'>
                        Confidence: {confidence*100:.1f}% ‚Ä¢ Analyzed {len(claims_to_check)} claims
                    </div>
                </div>
            """, unsafe_allow_html=True)

            # Claims Section
            if claims_to_check:
                with st.expander("üìã Extracted Claims", expanded=True):
                    for i, claim in enumerate(claims_to_check, 1):
                        st.markdown(f"<div style='margin: 0.5rem 0;'>‚ñ™Ô∏è {claim}</div>", unsafe_allow_html=True)

            # Fact-Check Results
            if fact_check_results:
                with st.expander("üîç Detailed Fact-Check Results", expanded=True):
                    for result in fact_check_results:
                        verdict_color = "#cc0000" if result['verdict'].lower() == "false" else "#228B22"
                        st.markdown(f"""
                            <div style='padding: 1rem; margin: 0.5rem 0; border-left: 4px solid {verdict_color}; background: white; border-radius: 4px;'>
                                <div style='font-weight: 600;'>{result['claim']}</div>
                                <div style='color: {verdict_color}; margin: 0.3rem 0;'>
                                    ‚öñÔ∏è {result['verdict']} (by {result['publisher']})
                                </div>
                                <a href='{result['url']}' target='_blank' style='color: #4a90e2; text-decoration: none;'>üîó View Source</a>
                            </div>
                        """, unsafe_allow_html=True)
            else:
                st.info("No external fact-check results found for the extracted claims.")

# --- Enhanced Sidebar ---
with st.sidebar:
    st.markdown("""
        <div style='padding: 1rem; background: #1a3345; border-radius: 8px; color: white;'>
            <h3 style='color: white;'>About This App</h3>
            <div style='margin: 1rem 0;'>
                <strong>Model Architecture:</strong><br>
                LinearSVC with TF-IDF vectorization<br>
                <strong>Accuracy:</strong> 99.8% (test set)<br>
                <strong>Last Updated:</strong> June 2025
            </div>
            <div style='margin: 1rem 0;'>
                <a href='https://github.com/yourusername/yourrepo' target='_blank' style='color: white; text-decoration: none;'>
                    <div style='padding: 0.5rem; background: #4a90e2; border-radius: 4px; text-align: center;'>
                        Visit GitHub Repo
                    </div>
                </a>
            </div>
        </div>
    """, unsafe_allow_html=True)
